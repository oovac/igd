
\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[margin=2.5cm]{geometry}

\begin{document}

\section*{Axioms of the Information--Focus Framework (v1.0)}

\begin{description}

\item[A1 (Distinguishability and events).]
There exists a set of states $\mathcal{S}$ and a $\sigma$-algebra of events on it. At least two states in $\mathcal{S}$ are distinguishable.

\item[A2 (Plausibility as probability).]
Uncertainty over events is represented by a real, normalized, additive measure $P$ on the event $\sigma$-algebra. This induces the standard probability calculus (Cox/Kolmogorov).

\item[A3 (Composition).]
For systems $A$ and $B$ there exists a composite system $A \otimes B$ with states and events such that independent subsystems factorize (product structure for independent states).

\item[A4 (Entropy as a functional).]
On probability distributions there exists a functional $H$ satisfying the Shannon--Khinchin axioms: continuity, maximality on the uniform distribution for a fixed support size, expandability, and additivity for independent systems.

\item[A5 (Processes as channels that do not inflate information).]
Any admissible process is a (classical or quantum) channel $\Phi$ that maps states to states and contractively transforms a suitable divergence $D$:
\[
    D(\Phi p \parallel \Phi q) \le D(p \parallel q)
\]
for all admissible states $p,q$.

\item[A6 (Resource cost of logical irreversibility).]
Any logically irreversible transformation (such as erasure of bits) requires a minimal exchange of resources with an environment (in standard thermodynamic realizations: at least $k_B T \ln 2$ of dissipated heat per erased bit).

\item[A7 (Computational boundedness).]
There exists a universal computational representation and a corresponding Kolmogorov complexity $K(x)$ for finite strings $x$, well defined up to an additive constant between universal machines. There exist incompressible strings whose shortest description has length asymptotically equal to $|x|$.

\item[A8 (Focus as projection).]
There exists a family of focus operators $\mathcal{F}_\theta$ that project or restrict a full state description onto a submanifold $\mathcal{M}_\theta$ (a model class). These focus maps obey a data processing inequality (they do not increase mutual information) and admit an information-geometric description via a Fisher metric on $\mathcal{M}_\theta$.

\end{description}

\section*{Fundamental Relations (Catalog v1.0)}

We list the core information-theoretic relations that follow from A1--A8 and constitute the minimal catalog of fundamental formulas.

\subsection*{Entropy and information}

\begin{description}

\item[F1 (Shannon entropy).]
For a discrete distribution $p = (p_i)$,
\[
    H(p) = - \sum_i p_i \log p_i .
\]

\item[F2 (Maximum entropy).]
For a distribution on $\Omega$ equiprobable states,
\[
    H_{\max} = \log \Omega .
\]

\item[F3 (Chain rule for entropy).]
For random variables $X,Y$,
\[
    H(X,Y) = H(X \mid Y) + H(Y) .
\]

\item[F4 (Mutual information).]
For $X,Y$,
\[
    I(X;Y) = H(X) + H(Y) - H(X,Y) \ge 0 .
\]

\item[F5 (Data Processing Inequality for mutual information).]
For any channel (or focus map) $\Phi$,
\[
    I(\Phi(X);Y) \le I(X;Y) .
\]

\end{description}

\subsection*{Divergences and contraction}

\begin{description}

\item[F6 (Kullback--Leibler divergence).]
For distributions $p,q$ with $p$ absolutely continuous w.r.t.\ $q$,
\[
    D_{\mathrm{KL}}(p \parallel q)
    = \sum_i p_i \log \frac{p_i}{q_i} \ge 0 ,
\]
with equality iff $p=q$.

\item[F7 (Contraction of KL under channels).]
For any admissible channel $\Phi$,
\[
    D_{\mathrm{KL}}(\Phi p \parallel \Phi q)
    \le D_{\mathrm{KL}}(p \parallel q) .
\]

\item[F8 (Strong subadditivity of entropy).]
For random variables $X,Y,Z$,
\[
    H(X,Y) + H(Y,Z) \ge H(Y) + H(X,Y,Z) .
\]

\end{description}

\subsection*{Information geometry and correlations}

\begin{description}

\item[F9 (Fisher information metric and local divergence).]
For a parametric family $P_\theta$ with log-likelihood $\log p(x \mid \theta)$, the Fisher information metric is
\[
    g_{ij}(\theta)
    = \mathbb{E}\big[ \partial_i \log p(x \mid \theta)\, \partial_j \log p(x \mid \theta) \big] \succeq 0 ,
\]
and locally
\[
    D(P_\theta \parallel P_{\theta + d\theta})
    \approx \tfrac{1}{2} d\theta^\top g(\theta) \, d\theta .
\]

\item[F10 (Bayes' rule).]
For hypothesis $h$ and data $d$,
\[
    P(h \mid d) = \frac{P(d \mid h) P(h)}{P(d)} \propto P(d \mid h) P(h) .
\]

\item[F11 (Total correlation / multi-information).]
For variables $X_1,\dots,X_n$,
\[
    T(X_{1:n}) = \sum_{i=1}^n H(X_i) - H(X_{1:n}) \ge 0 .
\]

\end{description}

\subsection*{Thermodynamic and resource bounds}

\begin{description}

\item[F12 (Landauer bound).]
For erasure of $\text{bits}$ bits of information at temperature $T$, the minimal entropy increase of the environment satisfies
\[
    \Delta S_{\text{env}} \ge \text{bits} \cdot k_B \ln 2 ,
\]
and the minimal work cost obeys
\[
    W_{\min} \ge \text{bits} \cdot k_B T \ln 2 .
\]

\item[F13 (Entropy production rate).]
For an open system with entropy change $\dot S_{\text{sys}}$ and entropy flow to the environment $\dot S_{\text{env}}$, the entropy production rate is
\[
    \sigma = \dot S_{\text{sys}} + \dot S_{\text{env}} \ge 0 .
\]

\end{description}

\subsection*{Computational limits and coding}

\begin{description}

\item[F14 (Kolmogorov complexity invariance and incompressibility).]
For universal machines $U,V$ there exists a constant $c_{UV}$ such that
\[
    K_U(x) \le K_V(x) + c_{UV}
\]
for all strings $x$, and there exist strings $x$ such that
\[
    K(x) \ge |x| - c
\]
for some fixed constant $c$ (incompressible strings).

\item[F15 (Kraft--McMillan inequality).]
For any prefix-free code with codeword lengths $l_i$,
\[
    \sum_i 2^{-l_i} \le 1 .
\]

\end{description}

\end{document}
